========================
TODO list for Dynamo 1.3
========================

(*) = done


1. Sample/reference problem set
     (1) Include all the reference problems from Dynamo 1.2
      2. Additional sample problems for dissipative/partial systems
            * Must cover all task types
      3. Example of optimizing taus

2. Minimal benchmarking framework
      1. Allow user to easily compare several algorithms for a given problem
      2. Produce spaghetti plots

3. Robust data provenance, reproducibility
     (1) Store in the dataset all necessary metadata, such as
	 Dynamo version, a timestamp, author name, description string etc.
            * Going from description to H is easy, H to description hard/impossible.
      2. Store the optimization parameters, initial sequence etc. so
         the exact same optimization can be re-run any time. Add a
	 function for this.
     (3) In the system description, include also dimension vector and
         labels for the states and controls.

4. Controls: transformations, limits, nonlinearity etc.
     (1) Implementation method: "fold" controls through a linearly scaled "sin" function
     (2) taus as optimization parameters
      3. Nondiagonal transformations, i.e. x = A cos(B), y = A sin(B). 
      4. Shai's Jacobian stack?
      5. Preconditioning?

5. Additional documentation
      1. Inline in the code
             * Modify some variable names to be self-documenting
      2. At function headers so Matlab's "help" gives more meaningful results
      3. Wiki
      4. Additional examples, see also 1.2
      5. Add reference to github for those wishing to contribute
      6. dynamo_manual.tex

6. More optimization tasks
      1. Closed S+E, where we only care about S
           * (unitary gates), state transfer
      2. Open S+E, Markovian evolution, where we only care about S.
           * unitary gates/general maps, state transfer

7. More algorithms
      1. "Integrators"
          * (expm), (eigendecomposition), expv, ode solvers, t-DMRG?
      2. Gradients
          * (GRAPE), (exact, finite diff, 1st order)
      3. Optimizers
          * (k-BFGS, using fminunc), Newton-Raphson, Monotonic Krotov variant?, Simplex? (no gradient needed)

8. Miscellaneous
      1. Strange bases for controls: CRAB?




Random ideas and things to test:


- express a state transfer problem in a Hermitian basis => real
  vectors and matrices instead of complex ones, faster...

- Ask Frank about noncommuting dP/du algorithms

- consistent function interface: dim as first or second parameter?

- Module for nth-degree gradient approximation with repeated scaling
  and squaring: Fouquieres et al., J. Magn. Reson. 212, 412 (2011).
  Automatic choice of \epsilon for finite-difference gradient method, ibid.

- Augmented matrix exponential gradient module for open systems, Floether et al. (2012)

- performance-compare error_open and the approximation given by error_real

- compare top-curve-sweeping for determining optimal T with direct tau-optimizing

- Lots of profiling. why do we use almost 30% more time in task1 than Dynamo 1.2? recompute_timeslots is called more?!?
  we seem to need more optimization rounds, maybe sth wrong with the gradient?

- error_abs and error_abs2 seem to give similar performance (as they should) (roughly same N_eval).
